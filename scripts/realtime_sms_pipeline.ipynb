{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb77542",
   "metadata": {},
   "source": [
    "# Real-Time SMS Pipeline Using Spark Structured Streaming - LOCAL TESTING\n",
    "\n",
    "This notebook implements a real-time data pipeline that:\n",
    "1. Ingests SMS data from Kafka topics (FCDR and ECDR)\n",
    "2. Applies business logic transformations using Spark DataFrames\n",
    "3. Writes processed data to **LOCAL data lake** in Parquet format (`/home/homar/spark-kafka-stream/data_lake/`)\n",
    "4. Loads data into Vertica for querying\n",
    "\n",
    "## Pipeline Overview:\n",
    "- **Source**: Kafka topics for SMS events (FCDR Jasmin, FCDR Telestax, ECDR)\n",
    "- **Processing**: Spark Structured Streaming with 1-minute micro-batches\n",
    "- **Target**: LOCAL data lake (Parquet) → Vertica (standard.fact_sms table)\n",
    "\n",
    "## Local Testing Configuration:\n",
    "- **Data Lake Path**: `/home/homar/spark-kafka-stream/data_lake/`\n",
    "- **Checkpoints Path**: `/home/homar/spark-kafka-stream/checkpoints/`\n",
    "- **OCI Cloud Storage**: Commented out for local testing\n",
    "\n",
    "This pipeline replaces the complex Vertica-based ETL with a scalable Spark streaming solution, now configured for local development and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a209b7",
   "metadata": {},
   "source": [
    "## 1. Setup Spark Session and Kafka Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bf2cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa5a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "\n",
    "# Initialize Spark Session with optimized configurations for Local Data Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RealTime_SMS_Pipeline_Local\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.driver.port\", \"7077\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointFileManagerClass\", \"org.apache.spark.sql.execution.streaming.CheckpointFileManager\") \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.streaming.stateStore.maintenanceInterval\", \"600s\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark/jars/vertica-jdbc.jar\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/usr/local/spark/jars/vertica-jdbc.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/usr/local/spark/jars/vertica-jdbc.jar\") \\\n",
    "    # OCI Configuration - Commented out for local testing\n",
    "    # .config(\"spark.hadoop.fs.oci.client.hostname\", \"objectstorage.me-jeddah-1.oraclecloud.com\") \\\n",
    "    # .config(\"spark.hadoop.fs.oci.client.auth.tenantId\", \"ocid1.tenancy.oc1..aaaaaaaaxxxxx\") \\\n",
    "    # .config(\"spark.hadoop.fs.oci.client.auth.userId\", \"ocid1.user.oc1..aaaaaaaaxxxxx\") \\\n",
    "    # .config(\"spark.hadoop.fs.oci.client.auth.fingerprint\", \"xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx\") \\\n",
    "    # .config(\"spark.hadoop.fs.oci.client.auth.privateKeyFilePath\", \"/home/jovyan/code/oci_api_key.pem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Production Kafka Configuration - Updated for your production environment\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"strimzi-kafka-cluster-oci-prod-2-kafka-bootstrap.strimzi-kafka-prod-2:9092\"\n",
    "\n",
    "# Production Kafka Topics - Real topics from Unifonic production\n",
    "KAFKA_TOPICS = {\n",
    "    \"fcdr_jasmin\": [\n",
    "        \"src-aws-prod.prod.sms.fcdr.jasmin\",\n",
    "        \"src-aws-prod.prod.sms.fcdr.scl-jasmin\", \n",
    "        \"src-new-oci-prod.prod.sms.fcdr.trx-oci-jasmin\",\n",
    "        \"src-stc-prod.prod.sms.fcdr.jasmin\",\n",
    "        \"src-oci-dr-ruh-prod.prod.sms.fcdr.trx-oci-jasmin\"\n",
    "    ],\n",
    "    \"fcdr_telestax\": [\"oci.prod.sms.fcdr.telestax\"],\n",
    "    \"ecdr\": [\"prod.sms.ecdr.el\", \"src-aws-prod.prod.sms.ecdr.el\"]\n",
    "}\n",
    "\n",
    "# OCI Data Lake Configuration - Commented out for local testing\n",
    "# OCI_NAMESPACE = \"unifonic\"  # Replace with your OCI tenancy namespace\n",
    "# OCI_BUCKET = \"spark_data_lake\"  # Your existing data lake bucket\n",
    "# DATA_LAKE_PATH = f\"oci://{OCI_BUCKET}@{OCI_NAMESPACE}/sms_pipeline/\"\n",
    "# CHECKPOINT_PATH = f\"oci://{OCI_BUCKET}@{OCI_NAMESPACE}/checkpoints/\"\n",
    "\n",
    "# Local paths (for local testing) - NOW ACTIVE\n",
    "LOCAL_DATA_LAKE_PATH = \"/home/homar/spark-kafka-stream/data_lake/\"\n",
    "LOCAL_CHECKPOINT_PATH = \"/home/homar/spark-kafka-stream/checkpoints/\"\n",
    "\n",
    "# Use local paths as primary for testing\n",
    "DATA_LAKE_PATH = LOCAL_DATA_LAKE_PATH\n",
    "CHECKPOINT_PATH = LOCAL_CHECKPOINT_PATH\n",
    "\n",
    "print(\" Spark Session initialized successfully for Local Data Lake\")\n",
    "print(f\"Kafka Bootstrap Servers: {KAFKA_BOOTSTRAP_SERVERS}\")\n",
    "print(f\"Local Data Lake Path: {DATA_LAKE_PATH}\")\n",
    "print(f\"Local Checkpoint Path: {CHECKPOINT_PATH}\")\n",
    "print(f\"Pipeline will run every 1 minute\")\n",
    "print(\"Running in LOCAL testing environment!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8455ad9e",
   "metadata": {},
   "source": [
    "## 2. Define Kafka Topic Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc387ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schemas for parsing JSON data from Kafka topics\n",
    "\n",
    "# FCDR Jasmin Schema - nested JSON structure under \"data.body\"\n",
    "fcdr_jasmin_schema = StructType([\n",
    "    StructField(\"data\", StructType([\n",
    "        StructField(\"body\", StructType([\n",
    "            StructField(\"messageid\", StringType(), True),\n",
    "            StructField(\"messageDeliveryStatus\", StringType(), True),\n",
    "            StructField(\"submitDate\", StringType(), True),\n",
    "            StructField(\"deliveryDate\", StringType(), True),\n",
    "            StructField(\"addrSrcDigits\", StringType(), True),\n",
    "            StructField(\"addrDstDigits\", StringType(), True),\n",
    "            StructField(\"origNetworkId\", StringType(), True),\n",
    "            StructField(\"networkId\", StringType(), True),\n",
    "            StructField(\"messageType\", StringType(), True),\n",
    "            StructField(\"first20CharactersOfSms\", StringType(), True),\n",
    "            StructField(\"reasonForFailure\", StringType(), True),\n",
    "            StructField(\"elMessageId\", StringType(), True),\n",
    "            StructField(\"elCorrelationId\", StringType(), True),\n",
    "            StructField(\"campaignId\", StringType(), True),\n",
    "            StructField(\"mprocNotes18\", StringType(), True), # operator_id\n",
    "            StructField(\"mprocNotes19\", StringType(), True), # country_id\n",
    "            StructField(\"mprocNotes20\", StringType(), True), # provider_id\n",
    "            StructField(\"mprocNotes4\", StringType(), True),  # mnp_used\n",
    "            StructField(\"diameterSubscriptionId\", StringType(), True),\n",
    "            StructField(\"diameterResponseStatus\", StringType(), True),\n",
    "            StructField(\"normalizedReason\", StringType(), True),\n",
    "            StructField(\"normalizedStatus\", StringType(), True),\n",
    "            StructField(\"charNumbers\", StringType(), True),\n",
    "            StructField(\"udh1MessageIdOfFirstPartMessage\", StringType(), True),\n",
    "            StructField(\"udh2NumberOfParts\", StringType(), True),\n",
    "            StructField(\"udh3NumberOfCurrentPart\", StringType(), True),\n",
    "            StructField(\"completeMessageBodyFlag\", StringType(), True),\n",
    "            StructField(\"aiClassification\", StringType(), True)\n",
    "        ]), True)\n",
    "    ]), True),\n",
    "    StructField(\"time_ingested\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# FCDR Telestax Schema - flat structure\n",
    "fcdr_telestax_schema = StructType([\n",
    "    StructField(\"MessageId\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"SubmitDate\", StringType(), True),\n",
    "    StructField(\"RespTimestamp\", StringType(), True),\n",
    "    StructField(\"SourceAddr\", StringType(), True),\n",
    "    StructField(\"DestAddr\", StringType(), True),\n",
    "    StructField(\"OrigNetworkId\", StringType(), True),\n",
    "    StructField(\"NetworkId\", StringType(), True),\n",
    "    StructField(\"MessageOrDlr\", StringType(), True),\n",
    "    StructField(\"ShortMessageText20\", StringType(), True),\n",
    "    StructField(\"reason\", StringType(), True),\n",
    "    StructField(\"ELMessageId\", StringType(), True),\n",
    "    StructField(\"ELCorrelationId\", StringType(), True),\n",
    "    StructField(\"CampaignID\", StringType(), True),\n",
    "    StructField(\"mprocnotes\", StringType(), True),\n",
    "    StructField(\"UDHPartsCount\", StringType(), True),\n",
    "    StructField(\"UDHCurrentPartNumber\", StringType(), True),\n",
    "    StructField(\"UDHFirstMessageId\", StringType(), True),\n",
    "    StructField(\"CompleteMessageBodyFlag\", StringType(), True),\n",
    "    StructField(\"DiamSessionId\", StringType(), True),\n",
    "    StructField(\"DiamResponseStatus\", StringType(), True),\n",
    "    StructField(\"NormalizedReason\", StringType(), True),\n",
    "    StructField(\"NormalizedStatus\", StringType(), True),\n",
    "    StructField(\"CharNumbers\", StringType(), True),\n",
    "    StructField(\"time_ingested\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# ECDR Schema - nested JSON structure under \"data.cdrEvent\"\n",
    "ecdr_schema = StructType([\n",
    "    StructField(\"data\", StructType([\n",
    "        StructField(\"cdrEvent\", StructType([\n",
    "            StructField(\"messageId\", StringType(), True),\n",
    "            StructField(\"messageStatus\", StringType(), True),\n",
    "            StructField(\"timestamp\", StringType(), True),\n",
    "            StructField(\"timestampA\", StringType(), True),\n",
    "            StructField(\"senderName\", StringType(), True),\n",
    "            StructField(\"recipient\", StringType(), True),\n",
    "            StructField(\"userId\", StringType(), True),\n",
    "            StructField(\"masterAccountId\", StringType(), True),\n",
    "            StructField(\"networkId\", StringType(), True),\n",
    "            StructField(\"messageParts\", StringType(), True),\n",
    "            StructField(\"providerId\", StringType(), True),\n",
    "            StructField(\"messageBody\", StringType(), True),\n",
    "            StructField(\"rejectReason\", StringType(), True),\n",
    "            StructField(\"correlationId\", StringType(), True),\n",
    "            StructField(\"campaignId\", StringType(), True),\n",
    "            StructField(\"messageType\", StringType(), True),\n",
    "            StructField(\"mnpUsed\", StringType(), True),\n",
    "            StructField(\"diameterSessionId\", StringType(), True),\n",
    "            StructField(\"diameterResponseCode\", StringType(), True),\n",
    "            StructField(\"numOfChars\", StringType(), True),\n",
    "            StructField(\"traceId\", StringType(), True),\n",
    "            StructField(\"customerStatus\", StringType(), True),\n",
    "            StructField(\"customerMessage\", StringType(), True),\n",
    "            StructField(\"isEncrypted\", StringType(), True),\n",
    "            StructField(\"smsClassification\", StringType(), True)\n",
    "        ]), True)\n",
    "    ]), True),\n",
    "    StructField(\"tags\", StructType([\n",
    "        StructField(\"sourceIp\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"time_ingested\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "print(\" Schemas defined for FCDR Jasmin, FCDR Telestax, and ECDR topics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5472e54",
   "metadata": {},
   "source": [
    "## 3. Read Data from Kafka Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create Kafka streaming DataFrame for specific topics\n",
    "def create_kafka_stream(topics_list, stream_name):\n",
    "    \"\"\"Create a Kafka streaming DataFrame for given topics\"\"\"\n",
    "    topics_str = \",\".join(topics_list)\n",
    "    \n",
    "    kafka_df = spark.readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "        .option(\"subscribe\", topics_str) \\\n",
    "        .option(\"startingOffsets\", \"latest\") \\\n",
    "        .option(\"failOnDataLoss\", \"false\") \\\n",
    "        .load()\n",
    "    \n",
    "    # Add metadata columns and convert to string\n",
    "    parsed_df = kafka_df.selectExpr(\n",
    "        \"CAST(key AS STRING) as key\",\n",
    "        \"CAST(value AS STRING) as value\", \n",
    "        \"topic\",\n",
    "        \"partition\",\n",
    "        \"offset\",\n",
    "        \"timestamp as kafka_timestamp\"\n",
    "    ).withColumn(\"stream_name\", lit(stream_name))\n",
    "    \n",
    "    print(f\" Created streaming DataFrame for {stream_name}: {topics_str}\")\n",
    "    return parsed_df\n",
    "\n",
    "# Create streaming DataFrames for each topic group\n",
    "fcdr_jasmin_stream = create_kafka_stream(KAFKA_TOPICS[\"fcdr_jasmin\"], \"fcdr_jasmin\")\n",
    "fcdr_telestax_stream = create_kafka_stream(KAFKA_TOPICS[\"fcdr_telestax\"], \"fcdr_telestax\")  \n",
    "ecdr_stream = create_kafka_stream(KAFKA_TOPICS[\"ecdr\"], \"ecdr\")\n",
    "\n",
    "print(\"\\n All Kafka streaming DataFrames created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6129b966",
   "metadata": {},
   "source": [
    "## 4. Parse and Clean Raw Kafka Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee66c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse JSON data from Kafka value column\n",
    "\n",
    "def parse_fcdr_jasmin_data(df):\n",
    "    \"\"\"Parse FCDR Jasmin JSON data\"\"\"\n",
    "    parsed_df = df.select(\n",
    "        col(\"key\"),\n",
    "        from_json(col(\"value\"), fcdr_jasmin_schema).alias(\"parsed_data\"),\n",
    "        col(\"topic\"),\n",
    "        col(\"partition\"), \n",
    "        col(\"offset\"),\n",
    "        col(\"kafka_timestamp\"),\n",
    "        col(\"stream_name\")\n",
    "    ).select(\n",
    "        col(\"key\"),\n",
    "        col(\"parsed_data.data.body.*\"),\n",
    "        col(\"parsed_data.time_ingested\").alias(\"time_ingested\"),\n",
    "        col(\"topic\"),\n",
    "        col(\"partition\"),\n",
    "        col(\"offset\"), \n",
    "        col(\"kafka_timestamp\"),\n",
    "        col(\"stream_name\")\n",
    "    ).filter(col(\"messageType\") == \"message\")  # Only process messages, not DLRs\n",
    "    \n",
    "    return parsed_df\n",
    "\n",
    "def parse_fcdr_telestax_data(df):\n",
    "    \"\"\"Parse FCDR Telestax JSON data\"\"\"\n",
    "    parsed_df = df.select(\n",
    "        col(\"key\"),\n",
    "        from_json(col(\"value\"), fcdr_telestax_schema).alias(\"parsed_data\"),\n",
    "        col(\"topic\"),\n",
    "        col(\"partition\"),\n",
    "        col(\"offset\"),\n",
    "        col(\"kafka_timestamp\"), \n",
    "        col(\"stream_name\")\n",
    "    ).select(\n",
    "        col(\"key\"),\n",
    "        col(\"parsed_data.*\"),\n",
    "        col(\"topic\"),\n",
    "        col(\"partition\"),\n",
    "        col(\"offset\"),\n",
    "        col(\"kafka_timestamp\"),\n",
    "        col(\"stream_name\")\n",
    "    ).filter(col(\"MessageOrDlr\") == \"message\")  # Only process messages, not DLRs\n",
    "    \n",
    "    return parsed_df\n",
    "\n",
    "def parse_ecdr_data(df):\n",
    "    \"\"\"Parse ECDR JSON data\"\"\"\n",
    "    parsed_df = df.select(\n",
    "        col(\"key\"),\n",
    "        from_json(col(\"value\"), ecdr_schema).alias(\"parsed_data\"),\n",
    "        col(\"topic\"),\n",
    "        col(\"partition\"),\n",
    "        col(\"offset\"),\n",
    "        col(\"kafka_timestamp\"),\n",
    "        col(\"stream_name\")\n",
    "    ).select(\n",
    "        col(\"key\"),\n",
    "        col(\"parsed_data.data.cdrEvent.*\"),\n",
    "        col(\"parsed_data.tags.sourceIp\").alias(\"tags_sourceIp\"),\n",
    "        col(\"parsed_data.time_ingested\").alias(\"time_ingested\"),\n",
    "        col(\"topic\"),\n",
    "        col(\"partition\"),\n",
    "        col(\"offset\"),\n",
    "        col(\"kafka_timestamp\"),\n",
    "        col(\"stream_name\")\n",
    "    ).filter(col(\"messageStatus\").isin(\"Rejected\", \"Failed\", \"Duplicated\"))  # Only failed messages\n",
    "    \n",
    "    return parsed_df\n",
    "\n",
    "# Parse the streaming DataFrames\n",
    "fcdr_jasmin_parsed = parse_fcdr_jasmin_data(fcdr_jasmin_stream)\n",
    "fcdr_telestax_parsed = parse_fcdr_telestax_data(fcdr_telestax_stream)\n",
    "ecdr_parsed = parse_ecdr_data(ecdr_stream)\n",
    "\n",
    "print(\" Raw Kafka data parsed successfully for all streams\")\n",
    "print(\" FCDR Jasmin: Messages from Jasmin SMSC servers\")\n",
    "print(\" FCDR Telestax: Messages from Telestax SMSC servers\") \n",
    "print(\" ECDR: Failed/Rejected external layer events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ef429",
   "metadata": {},
   "source": [
    "## 5. Transform FCDR Messages Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b58879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform FCDR Jasmin messages according to business logic\n",
    "def transform_fcdr_jasmin_messages(df):\n",
    "    \"\"\"Apply transformations to FCDR Jasmin message data\"\"\"\n",
    "    \n",
    "    transformed_df = df.select(\n",
    "        # Generate natural key for deduplication\n",
    "        concat_ws(\"~\",\n",
    "            coalesce(col(\"messageid\"), lit(\"0\")),\n",
    "            coalesce(col(\"origNetworkId\"), lit(\"0\")), \n",
    "            coalesce(col(\"addrDstDigits\"), lit(\"0\")),\n",
    "            coalesce(date_format(col(\"submitDate\").cast(\"timestamp\"), \"yyyyMMddHHmmssSSS\"), lit(\"NONE\")),\n",
    "            coalesce(date_format(col(\"deliveryDate\").cast(\"timestamp\"), \"yyyyMMddHHmmssSSS\"), lit(\"NONE\")),\n",
    "            coalesce(col(\"networkId\"), lit(\"0\")),\n",
    "            coalesce(upper(col(\"messageDeliveryStatus\")), lit(\"NONE\")),\n",
    "            coalesce(col(\"udh3NumberOfCurrentPart\"), lit(\"0\"))\n",
    "        ).alias(\"natural_key\"),\n",
    "        \n",
    "        # Basic message fields\n",
    "        col(\"messageid\").cast(\"int\").alias(\"message_id\"),\n",
    "        \n",
    "        # Message status logic\n",
    "        when(upper(col(\"messageDeliveryStatus\")) == \"SUCCESS_ESME\", \"SENT\")\n",
    "        .when((upper(col(\"messageDeliveryStatus\")).isin(\"FAILED_ESME\", \"EL_FAILED\", \"EL_REJECTED\", \"EL_DUPLICATED\", \"FAILED\")) \n",
    "              & (col(\"diameterResponseStatus\") == \"2001\"), \"SENT\")\n",
    "        .otherwise(\"NOTSENT\").alias(\"message_status\"),\n",
    "        \n",
    "        # Date handling\n",
    "        coalesce(col(\"deliveryDate\").cast(\"timestamp\"), col(\"time_ingested\")).alias(\"submit_date\"),\n",
    "        col(\"submitDate\").cast(\"timestamp\").alias(\"user_submit_date\"),\n",
    "        coalesce(col(\"deliveryDate\").cast(\"timestamptz\"), col(\"time_ingested\").cast(\"timestamptz\")).alias(\"submit_date_tz\"),\n",
    "        \n",
    "        # Contact information\n",
    "        col(\"addrSrcDigits\").alias(\"sender_name\"),\n",
    "        substring(col(\"addrDstDigits\"), 1, 20).alias(\"receiver\"),\n",
    "        \n",
    "        # Network and account info\n",
    "        col(\"origNetworkId\").cast(\"int\").alias(\"account_id\"),\n",
    "        col(\"networkId\").cast(\"int\").alias(\"network_id\"),\n",
    "        \n",
    "        # SMS content\n",
    "        col(\"first20CharactersOfSms\").alias(\"short_message\"),\n",
    "        \n",
    "        # Business logic fields\n",
    "        upper(col(\"messageDeliveryStatus\")).alias(\"event_status\"),\n",
    "        \n",
    "        # DLR status logic \n",
    "        when(col(\"messageDeliveryStatus\") == \"success_esme\", \"PENDING\")\n",
    "        .when(col(\"messageDeliveryStatus\").isin(\"failed_Esme\", \"el_failed\", \"el_rejected\", \"el_duplicated\", \"mproc_rejected\", \"mproc_dropped\"), \"NOTDELIVERED\")\n",
    "        .otherwise(\"NONE\").alias(\"dlr_status\"),\n",
    "        \n",
    "        # Region logic\n",
    "        when(coalesce(col(\"mprocNotes19\"), lit(\"\")) != \"3\", \"INTERNATIONAL\")\n",
    "        .otherwise(\"LOCAL\").alias(\"region\"),\n",
    "        \n",
    "        # Unit count\n",
    "        coalesce(col(\"udh2NumberOfParts\").cast(\"int\"), lit(1)).alias(\"number_of_units\"),\n",
    "        \n",
    "        # Provider information from mproc notes | check only number only on those reguler expression Like (\"12345\"+\"98765\")\n",
    "        when(regexp_extract(col(\"mprocNotes18\"), \"^\\\\+?\\\\d+$\", 0) != \"\", col(\"mprocNotes18\").cast(\"bigint\"))\n",
    "        .otherwise(lit(0)).alias(\"operator_id\"),\n",
    "        \n",
    "        when(regexp_extract(col(\"mprocNotes19\"), \"^\\\\+?\\\\d+$\", 0) != \"\", col(\"mprocNotes19\").cast(\"bigint\"))\n",
    "        .otherwise(lit(0)).alias(\"country_id\"),\n",
    "        \n",
    "        when(regexp_extract(col(\"mprocNotes20\"), \"^\\\\+?\\\\d+$\", 0) != \"\", col(\"mprocNotes20\").cast(\"bigint\"))\n",
    "        .otherwise(lit(0)).alias(\"provider_id\"),\n",
    "        \n",
    "        # MNP flag\n",
    "        (col(\"mprocNotes4\") == \"Y\").alias(\"mnp_used\"),\n",
    "        \n",
    "        # Additional fields\n",
    "        col(\"reasonForFailure\").alias(\"failure\"),\n",
    "        col(\"elMessageId\").alias(\"el_message_id\"),\n",
    "        col(\"elCorrelationId\").alias(\"correlation_id\"),\n",
    "        col(\"campaignId\").alias(\"campaign_id\"),\n",
    "        col(\"charNumbers\").cast(\"int\").alias(\"message_body_length\"),\n",
    "        coalesce(col(\"diameterSubscriptionId\"), lit(\"null\")).alias(\"diameter_session_id\"),\n",
    "        col(\"diameterResponseStatus\").alias(\"diameter_response_status\"),\n",
    "        col(\"normalizedReason\").alias(\"normalized_reason\"),\n",
    "        col(\"normalizedStatus\").alias(\"normalized_status\"),\n",
    "        \n",
    "        # Source identification\n",
    "        lit(\"Jasmin\").alias(\"source_type\"),\n",
    "        lit(\"FCDR\").alias(\"cdr_kind\"),\n",
    "        \n",
    "        # Timestamps\n",
    "        current_timestamp().alias(\"created_time\"),\n",
    "        col(\"time_ingested\"),\n",
    "        \n",
    "        # SMS Classification\n",
    "        regexp_replace(col(\"aiClassification\"), '\"', '').alias(\"sms_classification\"),\n",
    "        \n",
    "        # Kafka metadata\n",
    "        col(\"topic\"),\n",
    "        col(\"partition\"),\n",
    "        col(\"offset\")\n",
    "    )\n",
    "    \n",
    "    return transformed_df\n",
    "\n",
    "# Transform FCDR Telestax messages according to business logic  \n",
    "def transform_fcdr_telestax_messages(df):\n",
    "    \"\"\"Apply transformations to FCDR Telestax message data\"\"\"\n",
    "    \n",
    "    transformed_df = df.select(\n",
    "        # Generate natural key\n",
    "        concat_ws(\"~\",\n",
    "            coalesce(col(\"MessageId\"), lit(\"0\")),\n",
    "            coalesce(col(\"OrigNetworkId\"), lit(\"0\")),\n",
    "            coalesce(col(\"DestAddr\"), lit(\"0\")),\n",
    "            coalesce(date_format(col(\"SubmitDate\").cast(\"timestamp\"), \"yyyyMMddHHmmssSSS\"), lit(\"NONE\")),\n",
    "            coalesce(date_format(col(\"RespTimestamp\").cast(\"timestamp\"), \"yyyyMMddHHmmssSSS\"), lit(\"NONE\")),\n",
    "            coalesce(col(\"NetworkId\"), lit(\"0\")),\n",
    "            coalesce(upper(col(\"Status\")), lit(\"NONE\")),\n",
    "            coalesce(col(\"UDHCurrentPartNumber\"), lit(\"0\"))\n",
    "        ).alias(\"natural_key\"),\n",
    "        \n",
    "        # Basic message fields\n",
    "        col(\"MessageId\").cast(\"int\").alias(\"message_id\"),\n",
    "        \n",
    "        # Message status logic\n",
    "        when(upper(col(\"Status\")) == \"SUCCESS_ESME\", \"SENT\")\n",
    "        .when((upper(col(\"Status\")).isin(\"FAILED_ESME\", \"EL_FAILED\", \"EL_REJECTED\", \"EL_DUPLICATED\", \"FAILED\"))\n",
    "              & (col(\"DiamResponseStatus\") == \"2001\"), \"SENT\")\n",
    "        .otherwise(\"NOTSENT\").alias(\"message_status\"),\n",
    "        \n",
    "        # Date handling\n",
    "        coalesce(col(\"RespTimestamp\").cast(\"timestamp\"), col(\"time_ingested\")).alias(\"submit_date\"),\n",
    "        col(\"SubmitDate\").cast(\"timestamp\").alias(\"user_submit_date\"), \n",
    "        coalesce(col(\"RespTimestamp\").cast(\"timestamptz\"), col(\"time_ingested\").cast(\"timestamptz\")).alias(\"submit_date_tz\"),\n",
    "        \n",
    "        # Contact information\n",
    "        col(\"SourceAddr\").alias(\"sender_name\"),\n",
    "        substring(col(\"DestAddr\"), 1, 20).alias(\"receiver\"),\n",
    "        \n",
    "        # Network and account info\n",
    "        col(\"OrigNetworkId\").cast(\"int\").alias(\"account_id\"),\n",
    "        col(\"NetworkId\").cast(\"int\").alias(\"network_id\"),\n",
    "        \n",
    "        # SMS content\n",
    "        col(\"ShortMessageText20\").alias(\"short_message\"),\n",
    "        \n",
    "        # Business logic fields\n",
    "        upper(col(\"Status\")).alias(\"event_status\"),\n",
    "        \n",
    "        # DLR status logic\n",
    "        when(upper(col(\"Status\")) == \"SUCCESS_ESME\", \"PENDING\")\n",
    "        .when(upper(col(\"Status\")).isin(\"FAILED_ESME\", \"EL_FAILED\", \"EL_REJECTED\", \"EL_DUPLICATED\", \"MPROC_REJECTED\", \"MPROC_DROPPED\"), \"NOTDELIVERED\")\n",
    "        .otherwise(\"NONE\").alias(\"dlr_status\"),\n",
    "        \n",
    "        # Region logic based on mprocnotes part 19\n",
    "        when(coalesce(split(col(\"mprocnotes\"), \":\").getItem(18), lit(\"\")) != \"3\", \"INTERNATIONAL\")\n",
    "        .otherwise(\"LOCAL\").alias(\"region\"),\n",
    "        \n",
    "        # Unit count\n",
    "        coalesce(col(\"UDHPartsCount\").cast(\"int\"), lit(1)).alias(\"number_of_units\"),\n",
    "        \n",
    "        # Provider information from mprocnotes\n",
    "        when(regexp_extract(split(col(\"mprocnotes\"), \":\").getItem(17), \"^\\\\+?\\\\d+$\", 0) != \"\", \n",
    "             split(col(\"mprocnotes\"), \":\").getItem(17).cast(\"bigint\"))\n",
    "        .otherwise(lit(0)).alias(\"operator_id\"),\n",
    "        \n",
    "        when(regexp_extract(split(col(\"mprocnotes\"), \":\").getItem(18), \"^\\\\+?\\\\d+$\", 0) != \"\",\n",
    "             split(col(\"mprocnotes\"), \":\").getItem(18).cast(\"bigint\"))\n",
    "        .otherwise(lit(0)).alias(\"country_id\"),\n",
    "        \n",
    "        when(regexp_extract(split(col(\"mprocnotes\"), \":\").getItem(19), \"^\\\\+?\\\\d+$\", 0) != \"\",\n",
    "             split(col(\"mprocnotes\"), \":\").getItem(19).cast(\"bigint\"))\n",
    "        .otherwise(lit(0)).alias(\"provider_id\"),\n",
    "        \n",
    "        # MNP flag\n",
    "        (split(col(\"mprocnotes\"), \":\").getItem(3) == \"Y\").alias(\"mnp_used\"),\n",
    "        \n",
    "        # Additional fields\n",
    "        col(\"reason\").alias(\"failure\"),\n",
    "        col(\"ELMessageId\").alias(\"el_message_id\"),\n",
    "        col(\"ELCorrelationId\").alias(\"correlation_id\"),\n",
    "        col(\"CampaignID\").alias(\"campaign_id\"),\n",
    "        col(\"CharNumbers\").cast(\"int\").alias(\"message_body_length\"),\n",
    "        coalesce(col(\"DiamSessionId\"), lit(\"null\")).alias(\"diameter_session_id\"),\n",
    "        col(\"DiamResponseStatus\").alias(\"diameter_response_status\"),\n",
    "        col(\"NormalizedReason\").alias(\"normalized_reason\"),\n",
    "        col(\"NormalizedStatus\").alias(\"normalized_status\"),\n",
    "        \n",
    "        # Source identification\n",
    "        lit(\"Telestax\").alias(\"source_type\"),\n",
    "        lit(\"FCDR\").alias(\"cdr_kind\"),\n",
    "        \n",
    "        # Timestamps\n",
    "        current_timestamp().alias(\"created_time\"),\n",
    "        col(\"time_ingested\"),\n",
    "        \n",
    "        # Kafka metadata\n",
    "        col(\"topic\"),\n",
    "        col(\"partition\"), \n",
    "        col(\"offset\")\n",
    "    )\n",
    "    \n",
    "    return transformed_df\n",
    "\n",
    "# Apply transformations\n",
    "fcdr_jasmin_transformed = transform_fcdr_jasmin_messages(fcdr_jasmin_parsed)\n",
    "fcdr_telestax_transformed = transform_fcdr_telestax_messages(fcdr_telestax_parsed)\n",
    "\n",
    "print(\" FCDR message transformations completed\")\n",
    "print(\" Applied business logic for message status, regions, providers, and DLR status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd3943",
   "metadata": {},
   "source": [
    "## 6. Transform ECDR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3000cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform ECDR data according to business logic\n",
    "\n",
    "# That regular expression \"^\\\\+?\\\\d+$\" means:\n",
    "\n",
    "# ^ → match from the start of the string.\n",
    "\n",
    "# \\\\+? → optionally (?) match a plus sign +.\n",
    "\n",
    "# The double backslash is because in Python strings you escape \\ as \\\\.\n",
    "\n",
    "# \\\\d+ → match one or more digits (0–9).\n",
    "\n",
    "# $ → match the end of the string.\n",
    "def transform_ecdr_data(df):\n",
    "    \"\"\"Apply transformations to ECDR data (External CDR events)\"\"\"\n",
    "    \n",
    "    transformed_df = df.select(\n",
    "        # Basic message fields\n",
    "        col(\"messageId\").cast(\"int\").alias(\"message_id\"),\n",
    "        \n",
    "        # Message status logic for ECDR\n",
    "        when(col(\"messageStatus\") == \"success_esme\", \"SENT\")\n",
    "        .when((col(\"messageStatus\").isin(\"Failed_esme\", \"Failed\", \"Rejected\", \"Duplicated\"))\n",
    "              & (coalesce(col(\"diameterResponseCode\"), lit(\"X\")  ) == \"2001\"), \"SENT\")\n",
    "        .otherwise(\"NOTSENT\").alias(\"message_status\"),\n",
    "        \n",
    "        # Date handling\n",
    "        coalesce(col(\"timestampA\").cast(\"timestamp\"), col(\"time_ingested\")).alias(\"submit_date\"),\n",
    "        col(\"timestamp\").cast(\"timestamp\").alias(\"user_submit_date\"),\n",
    "        coalesce(col(\"timestampA\").cast(\"timestamptz\"), col(\"time_ingested\").cast(\"timestamptz\")).alias(\"submit_date_tz\"),\n",
    "        \n",
    "        # Contact information  \n",
    "        col(\"senderName\").alias(\"sender_name\"),\n",
    "        col(\"recipient\").cast(\"string\").alias(\"receiver\"),\n",
    "        \n",
    "        # Account and network info (ECDR accounts get +20000 offset)\n",
    "        when((regexp_extract(col(\"userId\"), \"^\\\\+?\\\\d+$\", 0) != \"\") & (length(col(\"userId\")) >= 1),\n",
    "             col(\"userId\").cast(\"int\") + 20000)\n",
    "        .otherwise(lit(0)).alias(\"account_id\"),\n",
    "        \n",
    "        when((regexp_extract(col(\"masterAccountId\"), \"^\\\\+?\\\\d+$\", 0) != \"\") & (length(col(\"masterAccountId\")) >= 1),\n",
    "             col(\"masterAccountId\").cast(\"int\"))\n",
    "        .otherwise(lit(0)).alias(\"master_account_id\"),\n",
    "        \n",
    "        when((regexp_extract(col(\"networkId\"), \"^\\\\+?\\\\d+$\", 0) != \"\") & (length(col(\"networkId\")) >= 1),\n",
    "             col(\"networkId\").cast(\"int\"))\n",
    "        .otherwise(lit(0)).alias(\"network_id\"),\n",
    "        \n",
    "        # SMS content\n",
    "        regexp_replace(col(\"messageBody\"), '\"', '').alias(\"short_message\"),\n",
    "        \n",
    "        # Event status mapping for ECDR\n",
    "        when(col(\"messageStatus\") == \"Rejected\", \"EL_REJECTED\")\n",
    "        .when(col(\"messageStatus\") == \"Duplicated\", \"EL_DUPLICATED\") \n",
    "        .when(col(\"messageStatus\") == \"Failed\", \"EL_FAILED\")\n",
    "        .otherwise(col(\"messageStatus\")).alias(\"event_status\"),\n",
    "        \n",
    "        # Protocol type for ECDR\n",
    "        lit(\"HTTP\").alias(\"source_protocol\"),\n",
    "        \n",
    "        # Unit count\n",
    "        when((regexp_extract(col(\"messageParts\"), \"^\\\\+?\\\\d+$\", 0) != \"\") & (length(col(\"messageParts\")) >= 1),\n",
    "             col(\"messageParts\").cast(\"int\"))\n",
    "        .otherwise(lit(0)).alias(\"number_of_units\"),\n",
    "        \n",
    "        # Provider ID\n",
    "        when((regexp_extract(col(\"providerId\"), \"^\\\\+?\\\\d+$\", 0) != \"\") & (length(col(\"providerId\")) >= 1),\n",
    "             col(\"providerId\").cast(\"int\"))\n",
    "        .otherwise(lit(0)).alias(\"provider_id\"),\n",
    "        \n",
    "        # ECDR specific fields\n",
    "        lit(\"NONE\").alias(\"dlr_status\"),\n",
    "        lit(\"LOCAL\").alias(\"region\"),  # ECDR is typically local traffic\n",
    "        \n",
    "        # Additional fields\n",
    "        col(\"rejectReason\").alias(\"failure\"),\n",
    "        col(\"messageId\").alias(\"el_message_id\"),  # ECDR messageId is the EL message ID\n",
    "        col(\"correlationId\").alias(\"correlation_id\"),\n",
    "        col(\"campaignId\").alias(\"campaign_id\"),\n",
    "        col(\"messageType\").alias(\"message_type\"),\n",
    "        col(\"mnpUsed\").cast(\"boolean\").alias(\"mnp_used\"),\n",
    "        \n",
    "        when(col(\"diameterSessionId\").cast(\"string\") != \"null\", col(\"diameterSessionId\"))\n",
    "        .otherwise(lit(null)).alias(\"diameter_session_id\"),\n",
    "        \n",
    "        col(\"diameterResponseCode\").alias(\"diameter_response_status\"),\n",
    "        \n",
    "        when((regexp_extract(col(\"numOfChars\"), \"^\\\\+?\\\\d+$\", 0) != \"\") & (length(col(\"numOfChars\")) >= 1),\n",
    "             col(\"numOfChars\").cast(\"int\"))\n",
    "        .otherwise(lit(0)).alias(\"message_body_length\"),\n",
    "        \n",
    "        col(\"traceId\").alias(\"trace_id\"),\n",
    "        col(\"customerStatus\").alias(\"customer_status\"),\n",
    "        col(\"customerMessage\").alias(\"customer_reason\"),\n",
    "        \n",
    "        # Charge message ID for ECDR\n",
    "        when(regexp_extract(col(\"diameterSessionId\").cast(\"string\"), \"^-?\\\\d+$\", 0) != \"\",\n",
    "             col(\"diameterSessionId\").cast(\"int\"))\n",
    "        .otherwise(lit(null)).alias(\"charge_message_id\"),\n",
    "        \n",
    "        # Product classification logic\n",
    "        when(lower(split(col(\"correlationId\"), \":\").getItem(1)) == \"sms\",\n",
    "             initcap(split(col(\"correlationId\"), \":\").getItem(0)))\n",
    "        .when(col(\"campaignId\").isNotNull() & (col(\"campaignId\") != \"\"),\n",
    "             lit(\"Campaign\"))\n",
    "        .otherwise(lit(\"SMS_Native\")).alias(\"product\"),\n",
    "        \n",
    "        # Source identification\n",
    "        lit(\"kafka\").alias(\"ingestion_source\"),\n",
    "        lit(\"ECDR\").alias(\"cdr_kind\"),\n",
    "        \n",
    "        # Additional ECDR fields\n",
    "        col(\"userId\").alias(\"el_user_id\"),\n",
    "        col(\"tags_sourceIp\").alias(\"source_ip\"),\n",
    "        col(\"isEncrypted\").cast(\"int\").alias(\"encrypted\"),\n",
    "        \n",
    "        # Product ID extraction\n",
    "        when(lower(split(col(\"correlationId\"), \":\").getItem(0)) == \"flowstudio\",\n",
    "             split(col(\"correlationId\"), \":\").getItem(2))\n",
    "        .when(lower(split(col(\"correlationId\"), \":\").getItem(0)) == \"campaign\",\n",
    "             split(col(\"correlationId\"), \":\").getItem(2))\n",
    "        .otherwise(lit(null)).alias(\"product_id\"),\n",
    "        \n",
    "        # White network ID logic(0 --< refer to 0 means \"extract the whole match\" (not just a capture group).)\n",
    "        when(regexp_extract(col(\"networkId\"), \"^\\\\d+$\", 0) != \"\",\n",
    "             (col(\"networkId\").cast(\"int\").between(500, 599)) |  # White Campaign\n",
    "             (col(\"networkId\").cast(\"int\").between(10, 99)) |    # White OTP\n",
    "             (col(\"networkId\").cast(\"int\").between(200, 299)) |  # White OTP  \n",
    "             (col(\"networkId\").cast(\"int\").between(700, 799)) |  # White Campaign\n",
    "             (col(\"networkId\").cast(\"int\").between(400, 499)) |  # White OTP\n",
    "             (col(\"networkId\").cast(\"int\").between(600, 699)) |  # White OTP\n",
    "             (col(\"networkId\").cast(\"int\").between(800, 899)))   # White International\n",
    "        .otherwise(lit(false)).alias(\"white_network_id\"),\n",
    "        \n",
    "        # Ad sender name detection\n",
    "        (lower(col(\"senderName\")).like(\"ad-%\") | lower(col(\"senderName\")).like(\"%-ad\")).alias(\"ad_sender_name\"),\n",
    "        \n",
    "        # SMS Classification\n",
    "        regexp_replace(col(\"smsClassification\"), '\"', '').alias(\"sms_classification\"),\n",
    "        \n",
    "        # Timestamps\n",
    "        current_timestamp().alias(\"created_time\"),\n",
    "        col(\"time_ingested\"),\n",
    "        \n",
    "        # Kafka metadata\n",
    "        col(\"topic\"),\n",
    "        col(\"partition\"),\n",
    "        col(\"offset\")\n",
    "    )\n",
    "    \n",
    "    return transformed_df\n",
    "\n",
    "# Apply ECDR transformations\n",
    "ecdr_transformed = transform_ecdr_data(ecdr_parsed)\n",
    "\n",
    "print(\" ECDR transformations completed\")\n",
    "print(\" Applied business logic for external layer events (failed/rejected messages)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89686068",
   "metadata": {},
   "source": [
    "## 7. Consolidate Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042293f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate all traffic data into unified schema\n",
    "def create_unified_fact_schema(fcdr_jasmin_df, fcdr_telestax_df, ecdr_df):\n",
    "    \"\"\"Create unified fact table schema from all data sources\"\"\"\n",
    "    \n",
    "    # Common columns for all traffic types\n",
    "    common_columns = [\n",
    "        \"message_id\", \"message_status\", \"submit_date\", \"sender_name\", \"receiver\",\n",
    "        \"event_status\", \"account_id\", \"network_id\", \"number_of_units\", \n",
    "        \"operator_id\", \"country_id\", \"provider_id\", \"dlr_status\", \"region\",\n",
    "        \"short_message\", \"failure\", \"el_message_id\", \"correlation_id\", \n",
    "        \"campaign_id\", \"mnp_used\", \"diameter_session_id\", \"diameter_response_status\",\n",
    "        \"normalized_reason\", \"normalized_status\", \"user_submit_date\", \n",
    "        \"message_body_length\", \"source_type\", \"cdr_kind\", \"created_time\", \n",
    "        \"time_ingested\", \"submit_date_tz\", \"sms_classification\",\n",
    "        \"topic\", \"partition\", \"offset\"\n",
    "    ]\n",
    "    \n",
    "    # Standardize FCDR Jasmin DataFrame\n",
    "    fcdr_jasmin_unified = fcdr_jasmin_df.select(\n",
    "        *common_columns,\n",
    "        lit(null).cast(\"string\").alias(\"source_protocol\"),\n",
    "        lit(null).cast(\"int\").alias(\"master_account_id\"), \n",
    "        lit(null).cast(\"string\").alias(\"message_type\"),\n",
    "        lit(null).cast(\"string\").alias(\"trace_id\"),\n",
    "        lit(null).cast(\"int\").alias(\"charge_message_id\"),\n",
    "        lit(null).cast(\"string\").alias(\"product\"),\n",
    "        lit(null).cast(\"string\").alias(\"ingestion_source\"),\n",
    "        lit(null).cast(\"string\").alias(\"el_user_id\"),\n",
    "        lit(null).cast(\"string\").alias(\"source_ip\"),\n",
    "        lit(null).cast(\"int\").alias(\"encrypted\"),\n",
    "        lit(null).cast(\"string\").alias(\"product_id\"),\n",
    "        lit(null).cast(\"boolean\").alias(\"white_network_id\"),\n",
    "        lit(null).cast(\"boolean\").alias(\"ad_sender_name\"),\n",
    "        # Date fields for partitioning\n",
    "        date(col(\"submit_date\")).alias(\"date_nk\"),\n",
    "        hour(col(\"submit_date\")).alias(\"hour_nk\"),\n",
    "        minute(col(\"submit_date\")).alias(\"minute_nk\")\n",
    "    )\n",
    "    \n",
    "    # Standardize FCDR Telestax DataFrame\n",
    "    fcdr_telestax_unified = fcdr_telestax_df.select(\n",
    "        *common_columns,\n",
    "        lit(null).cast(\"string\").alias(\"source_protocol\"),\n",
    "        lit(null).cast(\"int\").alias(\"master_account_id\"),\n",
    "        lit(null).cast(\"string\").alias(\"message_type\"), \n",
    "        lit(null).cast(\"string\").alias(\"trace_id\"),\n",
    "        lit(null).cast(\"int\").alias(\"charge_message_id\"),\n",
    "        lit(null).cast(\"string\").alias(\"product\"),\n",
    "        lit(null).cast(\"string\").alias(\"ingestion_source\"),\n",
    "        lit(null).cast(\"string\").alias(\"el_user_id\"),\n",
    "        lit(null).cast(\"string\").alias(\"source_ip\"),\n",
    "        lit(null).cast(\"int\").alias(\"encrypted\"),\n",
    "        lit(null).cast(\"string\").alias(\"product_id\"),\n",
    "        lit(null).cast(\"boolean\").alias(\"white_network_id\"),\n",
    "        lit(null).cast(\"boolean\").alias(\"ad_sender_name\"),\n",
    "        # Date fields for partitioning\n",
    "        date(col(\"submit_date\")).alias(\"date_nk\"),\n",
    "        hour(col(\"submit_date\")).alias(\"hour_nk\"),\n",
    "        minute(col(\"submit_date\")).alias(\"minute_nk\")\n",
    "    )\n",
    "    \n",
    "    # Standardize ECDR DataFrame\n",
    "    ecdr_unified = ecdr_df.select(\n",
    "        *common_columns,\n",
    "        col(\"source_protocol\"),\n",
    "        col(\"master_account_id\"),\n",
    "        col(\"message_type\"),\n",
    "        col(\"trace_id\"),\n",
    "        col(\"charge_message_id\"),\n",
    "        col(\"product\"),\n",
    "        col(\"ingestion_source\"),\n",
    "        col(\"el_user_id\"),\n",
    "        col(\"source_ip\"),\n",
    "        col(\"encrypted\"),\n",
    "        col(\"product_id\"),\n",
    "        col(\"white_network_id\"),\n",
    "        col(\"ad_sender_name\"),\n",
    "        # Date fields for partitioning\n",
    "        date(col(\"submit_date\")).alias(\"date_nk\"),\n",
    "        hour(col(\"submit_date\")).alias(\"hour_nk\"),\n",
    "        minute(col(\"submit_date\")).alias(\"minute_nk\")\n",
    "    )\n",
    "    \n",
    "    # Union all DataFrames\n",
    "    consolidated_df = fcdr_jasmin_unified.unionByName(fcdr_telestax_unified).unionByName(ecdr_unified)\n",
    "    \n",
    "    return consolidated_df\n",
    "\n",
    "# Create consolidated traffic DataFrame\n",
    "consolidated_traffic = create_unified_fact_schema(\n",
    "    fcdr_jasmin_transformed, \n",
    "    fcdr_telestax_transformed, \n",
    "    ecdr_transformed\n",
    ")\n",
    "\n",
    "# Add additional business logic fields\n",
    "final_traffic = consolidated_traffic.select(\n",
    "    \"*\",\n",
    "    # Generate unique record ID\n",
    "    concat(\n",
    "        col(\"cdr_kind\"), lit(\"_\"),\n",
    "        col(\"message_id\"), lit(\"_\"),\n",
    "        col(\"account_id\"), lit(\"_\"),\n",
    "        col(\"receiver\"), lit(\"_\"),\n",
    "        unix_timestamp(col(\"submit_date\"))\n",
    "    ).alias(\"record_id\"),\n",
    "    \n",
    "    # Operator flag logic\n",
    "    when((col(\"event_status\") == \"mproc_dropped\") |\n",
    "         (col(\"event_status\") == \"failed_esme\") |\n",
    "         (col(\"event_status\") == \"failed\") |\n",
    "         ((col(\"event_status\") == \"success_esme\") & \n",
    "          (coalesce(col(\"failure\"), lit(\"X\")).isin(\"exist in DND list\", \"message is duplicate\") == False)),\n",
    "         lit(True))\n",
    "    .otherwise(lit(False)).alias(\"operator_flag\"),\n",
    "    \n",
    "    # Remarks for data quality\n",
    "    when(col(\"submit_date\") == col(\"time_ingested\"), \n",
    "         lit(\"submit_date populated with time_ingested\"))\n",
    "    .otherwise(lit(null)).alias(\"remarks\"),\n",
    "    \n",
    "    # Final message flag\n",
    "    when((col(\"short_message\").isNotNull()) &\n",
    "         (col(\"event_status\") != \"partial_esme\"),\n",
    "         lit(True))\n",
    "    .otherwise(lit(False)).alias(\"final_message\")\n",
    ")\n",
    "\n",
    "print(\" Traffic data consolidated successfully\")\n",
    "print(\" Combined FCDR Jasmin, FCDR Telestax, and ECDR into unified schema\")\n",
    "print(\" Added business logic for operator flags, remarks, and final message classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d342887",
   "metadata": {},
   "source": [
    "## 8. Write to Data Lake (Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70796793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write processed data to Local Data Lake in Parquet format\n",
    "\n",
    "# OCI Data Lake Function - Commented out for local testing\n",
    "# def write_to_oci_data_lake(df, output_path, checkpoint_path):\n",
    "#     \"\"\"Write streaming DataFrame to OCI data lake with partitioning\"\"\"\n",
    "#     \n",
    "#     # Add partitioning columns\n",
    "#     partitioned_df = df.withColumn(\"year\", year(col(\"submit_date\"))) \\\n",
    "#                        .withColumn(\"month\", month(col(\"submit_date\"))) \\\n",
    "#                        .withColumn(\"day\", dayofmonth(col(\"submit_date\"))) \\\n",
    "#                        .withColumn(\"hour\", hour(col(\"submit_date\"))) \\\n",
    "#                        .withColumn(\"minute\", minute(col(\"submit_date\")))\n",
    "#     \n",
    "#     # Write to OCI data lake with partitioning and checkpointing\n",
    "#     query = partitioned_df.writeStream \\\n",
    "#         .format(\"parquet\") \\\n",
    "#         .option(\"path\", output_path) \\\n",
    "#         .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "#         .partitionBy(\"year\", \"month\", \"day\", \"hour\", \"minute\") \\\n",
    "#         .outputMode(\"append\") \\\n",
    "#         .trigger(processingTime=\"1 minute\") \\\n",
    "#         .option(\"maxFilesPerTrigger\", \"1000\") \\\n",
    "#         .option(\"maxRecordsPerFile\", \"100000\") \\\n",
    "#         .start()\n",
    "#     \n",
    "#     return query\n",
    "\n",
    "def write_to_local_data_lake(df, output_path, checkpoint_path):\n",
    "    \"\"\"Write streaming DataFrame to local data lake with partitioning\"\"\"\n",
    "    \n",
    "    # Add partitioning columns\n",
    "    partitioned_df = df.withColumn(\"year\", year(col(\"submit_date\"))) \\\n",
    "                       .withColumn(\"month\", month(col(\"submit_date\"))) \\\n",
    "                       .withColumn(\"day\", dayofmonth(col(\"submit_date\"))) \\\n",
    "                       .withColumn(\"hour\", hour(col(\"submit_date\"))) \\\n",
    "                       .withColumn(\"minute\", minute(col(\"submit_date\")))\n",
    "    \n",
    "    # Write to local storage\n",
    "    query = partitioned_df.writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"path\", output_path) \\\n",
    "        .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "        .partitionBy(\"year\", \"month\", \"day\", \"hour\", \"minute\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .trigger(processingTime=\"1 minute\") \\\n",
    "        .option(\"maxFilesPerTrigger\", \"1000\") \\\n",
    "        .start()\n",
    "    \n",
    "    return query\n",
    "\n",
    "# Set up paths for local data lake output\n",
    "fact_sms_output_path = f\"{DATA_LAKE_PATH}fact_sms/\"\n",
    "fact_sms_checkpoint_path = f\"{CHECKPOINT_PATH}fact_sms_checkpoint/\"\n",
    "\n",
    "print(\" Setting up LOCAL data lake write operations...\")\n",
    "print(f\"Output path: {fact_sms_output_path}\")\n",
    "print(f\"Checkpoint path: {fact_sms_checkpoint_path}\")\n",
    "print(\"Trigger: Every 1 minute\")\n",
    "print(\"Format: Parquet with partitioning by year/month/day/hour/minute\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "import os\n",
    "os.makedirs(fact_sms_output_path, exist_ok=True)\n",
    "os.makedirs(fact_sms_checkpoint_path, exist_ok=True)\n",
    "\n",
    "# Write directly to local storage (no OCI fallback needed)\n",
    "print(\"Writing to LOCAL Data Lake...\")\n",
    "\n",
    "# Start streaming query to write to local storage\n",
    "data_lake_query = write_to_local_data_lake(\n",
    "    final_traffic,\n",
    "    fact_sms_output_path,\n",
    "    fact_sms_checkpoint_path\n",
    ")\n",
    "\n",
    "print(\"Local data lake streaming query started successfully\")\n",
    "print(f\"Data will be written to {fact_sms_output_path} every minute\")\n",
    "print(\"You can monitor the parquet files in your local directory\")\n",
    "\n",
    "# OCI Code - Commented out for local testing\n",
    "# try:\n",
    "#     print(\" Attempting to write to OCI Data Lake...\")\n",
    "#     \n",
    "#     # Test OCI connectivity by trying to write a small test file\n",
    "#     test_df = spark.createDataFrame([(\"test\",)], [\"value\"])\n",
    "#     test_path = f\"{DATA_LAKE_PATH}test/\"\n",
    "#     test_df.write.mode(\"overwrite\").parquet(test_path)\n",
    "#     \n",
    "#     print(\" OCI Data Lake is accessible - using cloud storage\")\n",
    "#     \n",
    "#     # Start streaming query to write to OCI data lake\n",
    "#     data_lake_query = write_to_oci_data_lake(\n",
    "#         final_traffic,\n",
    "#         fact_sms_output_path,\n",
    "#         fact_sms_checkpoint_path\n",
    "#     )\n",
    "#     \n",
    "#     print(\" OCI Data lake streaming query started successfully\")\n",
    "#     print(\"  Data will be written to OCI bucket 'spark_data_leak' every minute\")\n",
    "#     \n",
    "# except Exception as e:\n",
    "#     print(f\"OCI Data Lake not accessible: {e}\")\n",
    "#     print(\"Falling back to local storage for testing...\")\n",
    "# \n",
    "#     # Start streaming query to write to local storage\n",
    "#     data_lake_query = write_to_local_fallback(\n",
    "#         final_traffic,\n",
    "#         local_fact_sms_output_path,\n",
    "#         local_fact_sms_checkpoint_path\n",
    "#     )\n",
    "#     \n",
    "#     print(\"Local data lake streaming query started\")\n",
    "#     print(\"Data will be written to local storage every minute\")\n",
    "#     print(\"Remember to configure OCI credentials for production use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70a345",
   "metadata": {},
   "source": [
    "## 9. Load to Vertica Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e8d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Vertica Table Structure for Testing\n",
    "import jaydebeapi\n",
    "\n",
    "def setup_vertica_table():\n",
    "    \"\"\"Create the fact_sms table in Vertica container if it doesn't exist\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Connect to Vertica using jaydebeapi\n",
    "        conn = jaydebeapi.connect(\n",
    "            \"com.vertica.jdbc.Driver\",\n",
    "            VERTICA_CONFIG[\"url\"],\n",
    "            [VERTICA_CONFIG[\"user\"], VERTICA_CONFIG[\"password\"]],\n",
    "            \"/usr/local/spark/jars/vertica-jdbc.jar\"\n",
    "        )\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create schema if not exists\n",
    "        cursor.execute(\"CREATE SCHEMA IF NOT EXISTS standard;\")\n",
    "        \n",
    "        # Create fact_sms table\n",
    "        create_table_sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS standard.fact_sms (\n",
    "            record_id VARCHAR(500),\n",
    "            cdr_kind VARCHAR(10),\n",
    "            date_nk DATE,\n",
    "            hour_nk INTEGER,\n",
    "            message_id INTEGER,\n",
    "            message_status VARCHAR(20),\n",
    "            submit_date TIMESTAMP,\n",
    "            sender_name VARCHAR(50),\n",
    "            receiver VARCHAR(50),\n",
    "            event_status VARCHAR(50),\n",
    "            source_protocol VARCHAR(20),\n",
    "            account_id INTEGER,\n",
    "            master_account_id INTEGER,\n",
    "            network_id INTEGER,\n",
    "            number_of_units INTEGER,\n",
    "            provider_id INTEGER,\n",
    "            dlr_status VARCHAR(20),\n",
    "            region VARCHAR(20),\n",
    "            short_message VARCHAR(200),\n",
    "            failure VARCHAR(200),\n",
    "            el_message_id VARCHAR(100),\n",
    "            correlation_id VARCHAR(200),\n",
    "            campaign_id VARCHAR(100),\n",
    "            message_type VARCHAR(50),\n",
    "            mnp_used BOOLEAN,\n",
    "            diameter_session_id VARCHAR(100),\n",
    "            diameter_response_status VARCHAR(20),\n",
    "            user_submit_date TIMESTAMP,\n",
    "            message_body_length INTEGER,\n",
    "            trace_id VARCHAR(100),\n",
    "            created_time TIMESTAMP,\n",
    "            time_ingested TIMESTAMP,\n",
    "            submit_date_tz TIMESTAMPTZ,\n",
    "            remarks VARCHAR(200),\n",
    "            operator_flag BOOLEAN,\n",
    "            final_message BOOLEAN,\n",
    "            charge_message_id INTEGER,\n",
    "            product VARCHAR(50),\n",
    "            ingestion_source VARCHAR(50),\n",
    "            el_user_id VARCHAR(50),\n",
    "            source_ip VARCHAR(50),\n",
    "            encrypted INTEGER,\n",
    "            product_id VARCHAR(50),\n",
    "            white_network_id BOOLEAN,\n",
    "            ad_sender_name BOOLEAN,\n",
    "            sms_classification VARCHAR(50)\n",
    "        )\n",
    "        ORDER BY submit_date\n",
    "        SEGMENTED BY HASH(record_id) ALL NODES;\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(create_table_sql)\n",
    "        conn.commit()\n",
    "        \n",
    "        print(\"Vertica table standard.fact_sms created successfully\")\n",
    "        print(\"Table is ready to receive streaming data\")\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up Vertica table: {e}\")\n",
    "        print(\"Please ensure Vertica container is running and accessible\")\n",
    "\n",
    "# Run table setup\n",
    "setup_vertica_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data to Vertica standard.fact_sms table\n",
    "\n",
    "# Vertica connection configuration for Docker container\n",
    "VERTICA_CONFIG = {\n",
    "    \"url\": \"jdbc:vertica://ci-vertica-db:5433/customer_insights\",\n",
    "    \"user\": \"customer_insights\",\n",
    "    \"password\": \"customer_insights1\",\n",
    "    \"driver\": \"com.vertica.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "def write_to_vertica(batch_df, batch_id):\n",
    "    \"\"\"Write each micro-batch to Vertica using JDBC\"\"\"\n",
    "    \n",
    "    if batch_df.count() > 0:\n",
    "        print(f\"Processing batch {batch_id} with {batch_df.count()} records\")\n",
    "        \n",
    "        # Select only the columns that exist in Vertica fact_sms table\n",
    "        vertica_columns = [\n",
    "            \"record_id\", \"cdr_kind\", \"date_nk\", \"hour_nk\", \"message_id\", \n",
    "            \"message_status\", \"submit_date\", \"sender_name\", \"receiver\", \n",
    "            \"event_status\", \"source_protocol\", \"account_id\", \"master_account_id\",\n",
    "            \"network_id\", \"number_of_units\", \"provider_id\", \"dlr_status\", \n",
    "            \"region\", \"short_message\", \"failure\", \"el_message_id\", \n",
    "            \"correlation_id\", \"campaign_id\", \"message_type\", \"mnp_used\",\n",
    "            \"diameter_session_id\", \"diameter_response_status\", \"user_submit_date\",\n",
    "            \"message_body_length\", \"trace_id\", \"created_time\", \"time_ingested\",\n",
    "            \"submit_date_tz\", \"remarks\", \"operator_flag\", \"final_message\",\n",
    "            \"charge_message_id\", \"product\", \"ingestion_source\", \"el_user_id\",\n",
    "            \"source_ip\", \"encrypted\", \"product_id\", \"white_network_id\",\n",
    "            \"ad_sender_name\", \"sms_classification\"\n",
    "        ]\n",
    "        \n",
    "        # Select and prepare data for Vertica (unpacking)\n",
    "        vertica_df = batch_df.select(*vertica_columns)\n",
    "        \n",
    "        try:\n",
    "            # Write to Vertica using JDBC with upsert mode\n",
    "            vertica_df.write \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", VERTICA_CONFIG[\"url\"]) \\\n",
    "                .option(\"user\", VERTICA_CONFIG[\"user\"]) \\\n",
    "                .option(\"password\", VERTICA_CONFIG[\"password\"]) \\\n",
    "                .option(\"driver\", VERTICA_CONFIG[\"driver\"]) \\\n",
    "                .option(\"dbtable\", \"standard.fact_sms\") \\\n",
    "                .option(\"batchsize\", \"10000\") \\\n",
    "                .option(\"truncate\", \"false\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .save()\n",
    "            \n",
    "            print(f\"Batch {batch_id} successfully written to Vertica container\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing batch {batch_id} to Vertica: {e}\")\n",
    "            # Continue processing even if Vertica write fails\n",
    "    else:\n",
    "        print(f\"Batch {batch_id} is empty, skipping Vertica write\")\n",
    "\n",
    "# Alternative: Write to Vertica using foreachBatch for better control\n",
    "def create_vertica_writer(df, checkpoint_path):\n",
    "    \"\"\"Create streaming query that writes to Vertica\"\"\"\n",
    "    \n",
    "    query = df.writeStream \\\n",
    "        .foreachBatch(write_to_vertica) \\\n",
    "        .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .trigger(processingTime=\"1 minute\") \\\n",
    "        .start()\n",
    "    \n",
    "    return query\n",
    "\n",
    "# Set up Vertica checkpoint path\n",
    "vertica_checkpoint_path = f\"{CHECKPOINT_PATH}vertica_checkpoint/\"\n",
    "\n",
    "print(\"Setting up Vertica write operations for Docker container...\")\n",
    "print(f\"Vertica URL: {VERTICA_CONFIG['url']}\")\n",
    "print(f\"Target table: standard.fact_sms\")\n",
    "print(f\"Checkpoint path: {vertica_checkpoint_path}\")\n",
    "print(\"Connecting to Vertica container: ci-vertica-db\")\n",
    "\n",
    "# Start streaming query to write to Vertica\n",
    "vertica_query = create_vertica_writer(\n",
    "    final_traffic,\n",
    "    vertica_checkpoint_path         \n",
    ")\n",
    "\n",
    "print(\"Vertica streaming query started\")\n",
    "print(\"Data will be loaded to Vertica container every minute using JDBC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beff754e",
   "metadata": {},
   "source": [
    "## 10. Monitor Pipeline Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor streaming pipeline performance and health\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def monitor_streaming_queries():\n",
    "    \"\"\"Monitor the health and performance of streaming queries\"\"\"\n",
    "    \n",
    "    # Get all active streaming queries\n",
    "    active_queries = spark.streams.active\n",
    "    \n",
    "    print(\"📊 STREAMING PIPELINE MONITORING\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"⏰ Timestamp: {datetime.now()}\")\n",
    "    print(f\"🔍 Active Queries: {len(active_queries)}\")\n",
    "    \n",
    "    for i, query in enumerate(active_queries):\n",
    "        print(f\"\\n📈 Query {i+1}: {query.name if query.name else 'Unnamed'}\")\n",
    "        print(f\"   ID: {query.id}\")\n",
    "        print(f\"   Status: {query.status}\")\n",
    "        \n",
    "        if query.status == \"ACTIVE\":\n",
    "            try:\n",
    "                # Get progress information\n",
    "                progress = query.lastProgress\n",
    "                if progress:\n",
    "                    print(f\"   Batch ID: {progress.get('batchId', 'N/A')}\")\n",
    "                    print(f\"   Input Rows: {progress.get('inputRowsPerSecond', 'N/A')}\")\n",
    "                    print(f\"   Processing Time: {progress.get('durationMs', {}).get('triggerExecution', 'N/A')} ms\")\n",
    "                    \n",
    "                    # Check for any sources\n",
    "                    sources = progress.get('sources', [])\n",
    "                    for source in sources:\n",
    "                        print(f\"   Source: {source.get('description', 'Unknown')}\")\n",
    "                        print(f\"   Input Rows: {source.get('inputRowsPerSecond', 'N/A')}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  Error getting progress: {e}\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  Query is not active!\")\n",
    "\n",
    "def data_quality_checks(df_sample):\n",
    "    \"\"\"Perform basic data quality checks on streaming data\"\"\"\n",
    "    \n",
    "    print(\"\\n🔍 DATA QUALITY CHECKS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    try:\n",
    "        # Basic counts\n",
    "        total_records = df_sample.count() if df_sample else 0\n",
    "        print(f\"📊 Total Records in Sample: {total_records}\")\n",
    "        \n",
    "        if total_records > 0:\n",
    "            # Check for null message IDs\n",
    "            null_message_ids = df_sample.filter(col(\"message_id\").isNull()).count()\n",
    "            print(f\"❌ Null Message IDs: {null_message_ids}\")\n",
    "            \n",
    "            # Check message status distribution\n",
    "            status_dist = df_sample.groupBy(\"message_status\").count().collect()\n",
    "            print(\"📈 Message Status Distribution:\")\n",
    "            for row in status_dist:\n",
    "                print(f\"   {row['message_status']}: {row['count']}\")\n",
    "            \n",
    "            # Check CDR kind distribution\n",
    "            cdr_dist = df_sample.groupBy(\"cdr_kind\").count().collect()\n",
    "            print(\"📈 CDR Kind Distribution:\")\n",
    "            for row in cdr_dist:\n",
    "                print(f\"   {row['cdr_kind']}: {row['count']}\")\n",
    "                \n",
    "            # Check for recent data\n",
    "            recent_data = df_sample.filter(\n",
    "                col(\"time_ingested\") > (current_timestamp() - expr(\"INTERVAL 5 MINUTES\"))\n",
    "            ).count()\n",
    "            print(f\"⏰ Records in Last 5 Minutes: {recent_data}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error in data quality checks: {e}\")\n",
    "\n",
    "# Function to get sample data for monitoring\n",
    "def get_sample_data():\n",
    "    \"\"\"Get sample data from the current stream for quality checks\"\"\"\n",
    "    try:\n",
    "        # Get recent data from data lake for monitoring\n",
    "        sample_path = f\"{DATA_LAKE_PATH}fact_sms/\"\n",
    "        \n",
    "        # Read recent parquet files for monitoring\n",
    "        current_time = datetime.now()\n",
    "        sample_df = spark.read.parquet(sample_path) \\\n",
    "            .filter(col(\"time_ingested\") > (current_timestamp() - expr(\"INTERVAL 10 MINUTES\"))) \\\n",
    "            .limit(1000)\n",
    "        \n",
    "        return sample_df\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Cannot read sample data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Monitor streaming queries\n",
    "monitor_streaming_queries()\n",
    "\n",
    "# Perform data quality checks\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "sample_data = get_sample_data()\n",
    "data_quality_checks(sample_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ PIPELINE MONITORING COMPLETE\")\n",
    "print(\"🔄 Pipeline is processing SMS data in real-time every minute\")\n",
    "print(\"📊 Monitor this cell regularly to check pipeline health\")\n",
    "print(\"⚠️  Watch for any error messages or quality issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4836f57",
   "metadata": {},
   "source": [
    "## Pipeline Summary & Next Steps\n",
    "\n",
    "### ✅ What This Pipeline Accomplishes:\n",
    "\n",
    "1. **Real-time Data Ingestion**: Reads from multiple Kafka topics (FCDR Jasmin, FCDR Telestax, ECDR)\n",
    "2. **Business Logic Transformation**: Applies the same business rules as the original SQL-based pipeline\n",
    "3. **Data Lake Storage**: Writes processed data to S3 in Parquet format with time-based partitioning\n",
    "4. **Vertica Integration**: Loads data into Vertica for querying and reporting\n",
    "5. **Monitoring**: Includes health checks and data quality monitoring\n",
    "\n",
    "### 🔧 Configuration Required:\n",
    "\n",
    "- **Kafka Configuration**: Update `KAFKA_BOOTSTRAP_SERVERS` with production values\n",
    "- **S3 Configuration**: Configure AWS credentials and S3 bucket access\n",
    "- **Vertica Configuration**: Update `VERTICA_CONFIG` with actual connection details\n",
    "- **Spark Configuration**: Tune memory and resource settings for production\n",
    "\n",
    "### 🚀 Deployment Options:\n",
    "\n",
    "1. **Local Development**: Run this notebook for testing and development\n",
    "2. **Spark Cluster**: Deploy as Spark application on cluster for production\n",
    "3. **Cloud Services**: Use managed Spark services (AWS EMR, Azure Synapse, etc.)\n",
    "4. **Container Deployment**: Package as Docker container for Kubernetes deployment\n",
    "\n",
    "### 📊 Benefits vs Original Pipeline:\n",
    "\n",
    "- **Scalability**: Spark can handle much higher throughput than Vertica ETL\n",
    "- **Real-time Processing**: 1-minute micro-batches vs batch processing\n",
    "- **Maintainability**: Simple DataFrame operations vs complex SQL\n",
    "- **Monitoring**: Built-in streaming metrics and health checks\n",
    "- **Cost Efficiency**: Reduces load on expensive Vertica resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b3da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Cleanup and Shutdown (Run when stopping the pipeline)\n",
    "\n",
    "def shutdown_pipeline():\n",
    "    \"\"\"Gracefully shutdown all streaming queries\"\"\"\n",
    "    \n",
    "    print(\"🛑 SHUTTING DOWN PIPELINE\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    active_queries = spark.streams.active\n",
    "    \n",
    "    for query in active_queries:\n",
    "        try:\n",
    "            print(f\"⏹️  Stopping query: {query.id}\")\n",
    "            query.stop()\n",
    "            print(f\"✅ Query {query.id} stopped successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error stopping query {query.id}: {e}\")\n",
    "    \n",
    "    print(\"🔄 Waiting for queries to fully stop...\")\n",
    "    \n",
    "    # Wait for all queries to stop\n",
    "    for query in active_queries:\n",
    "        try:\n",
    "            query.awaitTermination(timeout=30)  # Wait max 30 seconds\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\"✅ All streaming queries stopped\")\n",
    "    print(\"🧹 Pipeline shutdown complete\")\n",
    "\n",
    "# Uncomment the following line when you want to stop the pipeline\n",
    "# shutdown_pipeline()\n",
    "\n",
    "print(\"🚀 REAL-TIME SMS PIPELINE IS RUNNING\")\n",
    "print(\"=\" * 40)\n",
    "print(\"📡 Ingesting data from Kafka topics\")\n",
    "print(\"🔄 Processing every 1 minute\")\n",
    "print(\"💾 Writing to S3 data lake\")\n",
    "print(\"🗄️  Loading to Vertica\")\n",
    "print(\"📊 Monitor the pipeline health above\")\n",
    "print(\"\\n⚠️  To stop the pipeline, uncomment and run: shutdown_pipeline()\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-kafka-stream (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
